"""
TC-LOAD-003: Peak Concurrent Load Test
5 Heavy Users (ETL) + 3 Light Users (Superset UI)

–¶–µ–ª—å: –°–∏–º—É–ª—è—Ü–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø–∏–∫–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏
- Heavy users: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CSV, DAG#1, DAG#2 (–±–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏)
- Light users: —Ä–∞–±–æ—Ç–∞ —Å –≥–æ—Ç–æ–≤—ã–º–∏ –¥–∞—à–±–æ—Ä–¥–∞–º–∏ (—Ñ–∏–ª—å—Ç—Ä—ã, —ç–∫—Å–ø–æ—Ä—Ç, –Ω–∞–≤–∏–≥–∞—Ü–∏—è)
"""

import logging
import os
import random
import threading
import time
import urllib3
from datetime import datetime
from typing import Optional, List, Dict
from threading import Lock, Event

from locust import task, between, events

from common.auth import establish_session
from common.api.load_api import LoadApi
from common.api.object_api import ChartApi
from common.csv_utils import count_chunks, count_csv_lines
from common.managers import UserPool
from common.clickhouse_monitor import ClickHouseMonitor
from common.report_engine import MetricsCollector, ReportGenerator  # üÜï Unified reporting system
from config import CONFIG

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# ============================================================================
# –°–ï–ö–¶–ò–Ø 1: –ì–õ–û–ë–ê–õ–¨–ù–û–ï –°–û–°–¢–û–Ø–ù–ò–ï (–ö–û–û–†–î–ò–ù–ê–¶–ò–Ø)
# ============================================================================

class DashboardPool:
    """
    –û–±—â–∏–π –ø—É–ª –¥–∞—à–±–æ—Ä–¥–æ–≤ –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –º–µ–∂–¥—É Heavy –∏ Light users

    –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:
    - Heavy users –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—à–±–æ—Ä–¥–∞ ‚Üí –¥–æ–±–∞–≤–ª—è—é—Ç –≤ –ø—É–ª
    - Light users ‚Üí –±–µ—Ä—É—Ç —Å–ª—É—á–∞–π–Ω—ã–π –¥–∞—à–±–æ—Ä–¥ –∏–∑ –ø—É–ª–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã
    - –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ (threading.Lock + threading.Event)
    """

    def __init__(self):
        self.lock = Lock()
        self.dashboards: List[tuple] = []  # [(url, owner_username, created_timestamp)]
        self.event = Event()  # –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è –ø–æ—è–≤–ª–µ–Ω–∏—è –¥–∞—à–±–æ—Ä–¥–æ–≤

    def add(self, url: str, owner: str):
        """Heavy user —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–Ω—ã–π –¥–∞—à–±–æ—Ä–¥"""
        with self.lock:
            self.dashboards.append((url, owner, time.time()))
            self.event.set()  # –°–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–∂–∏–¥–∞—é—â–∏–º Light users
            print(f"[DashboardPool] Added dashboard from {owner}: {url}")

    def get_random(self) -> Optional[str]:
        """Light user –±–µ—Ä—ë—Ç —Å–ª—É—á–∞–π–Ω—ã–π –¥–∞—à–±–æ—Ä–¥ –¥–ª—è —Ä–∞–±–æ—Ç—ã"""
        with self.lock:
            if self.dashboards:
                return random.choice(self.dashboards)[0]  # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º URL
        return None

    def has_dashboards(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å—Ç—å –ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –¥–∞—à–±–æ—Ä–¥"""
        with self.lock:
            return len(self.dashboards) > 0

    def count(self) -> int:
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—à–±–æ—Ä–¥–æ–≤"""
        with self.lock:
            return len(self.dashboards)

    def wait_until_available(self, timeout=600) -> bool:
        """
        Light user –∂–¥—ë—Ç –ø–æ—è–≤–ª–µ–Ω–∏—è –¥–∞—à–±–æ—Ä–¥–æ–≤ –æ—Ç Heavy users
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç threading.Event –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ –¥–∞—à–±–æ—Ä–¥—ã –ø–æ—è–≤–∏–ª–∏—Å—å, False –µ—Å–ª–∏ —Ç–∞–π–º–∞—É—Ç
        """
        return self.event.wait(timeout)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π singleton
_dashboard_pool_003 = DashboardPool()


def get_dashboard_pool_003() -> DashboardPool:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π DashboardPool"""
    return _dashboard_pool_003


# ============================================================================
# –°–ï–ö–¶–ò–Ø 2: UNIFIED METRICS COLLECTOR
# ============================================================================
# –ò—Å–ø–æ–ª—å–∑—É–µ–º MetricsCollector –∏–∑ report_engine.py –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ—Ç—á—ë—Ç–Ω–æ—Å—Ç–∏
# –û–¥–∏–Ω collector —Å–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç –æ–±–æ–∏—Ö —Ç–∏–ø–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:
# - Heavy users: ETL –æ–ø–µ—Ä–∞—Ü–∏–∏ (—Å SLO validation –∏ baseline comparison)
# - Light users: Superset UI –æ–ø–µ—Ä–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
# ============================================================================

_metrics_collector_003 = MetricsCollector(test_name="TC-LOAD-003")


# ============================================================================
# üìä SLO DEFINITIONS FOR TC-LOAD-003 (Peak Load Test)
# ============================================================================
# TC-LOAD-003 –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –ü–ò–ö–û–í–û–ô –Ω–∞–≥—Ä—É–∑–∫–µ (5 Heavy + 3 Light)
# SLO –∫—Ä–∏—Ç–µ—Ä–∏–π –∏–∑ README.md: "–ù–µ –±–æ–ª–µ–µ √ó2 –æ—Ç baseline –º–µ—Ç—Ä–∏–∫"
#
# ‚öôÔ∏è –ö–ê–ö –ù–ê–°–¢–†–û–ò–¢–¨ –ü–û–°–õ–ï –ü–û–õ–£–ß–ï–ù–ò–Ø BASELINE:
#
# –®–ê–ì–ò –ù–ê–°–¢–†–û–ô–ö–ò:
# 1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ TC-LOAD-001 –∏ –ø–æ–ª—É—á–∏—Ç–µ baseline –º–µ—Ç—Ä–∏–∫–∏
# 2. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤ –æ—Ç—á–µ—Ç–µ TC-LOAD-001 –∑–Ω–∞—á–µ–Ω–∏—è P95 –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏
# 3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ SLO –¥–ª—è TC-LOAD-003 = P95_baseline * 2.0 (—É–¥–≤–æ–µ–Ω–∏–µ –¥–æ–ø—É—Å—Ç–∏–º–æ –ø—Ä–∏ –ø–∏–∫–µ)
# 4. –û–±–Ω–æ–≤–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∏–∂–µ
#
# –ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á–µ—Ç–∞:
#   TC-LOAD-001 –æ—Ç—á–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç "DAG #1 P95: 280.5s"
#   TC-LOAD-003 SLO = 280.5 * 2.0 = 561 —Å–µ–∫—É–Ω–¥
#   –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç: –ø—Ä–∏ –ø–∏–∫–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ –¥–æ–ø—É—Å—Ç–∏–º–æ –∑–∞–º–µ–¥–ª–µ–Ω–∏–µ –¥–æ √ó2
#
# üìå –í–ê–ñ–ù–û: SLO –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –¢–û–õ–¨–ö–û –∫ Heavy users (ETL –æ–ø–µ—Ä–∞—Ü–∏—è–º)!
# Light users - —ç—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞, –∏—Ö –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
# ============================================================================

# SLO #1: DAG #1 Duration –¥–ª—è Peak —Ç–µ—Å—Ç–∞ (Heavy users only)
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –í—Ä–µ–º—è –∏–º–ø–æ—Ä—Ç–∞ CSV –≤ ClickHouse –ø—Ä–∏ 5 Heavy + 3 Light –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
# üéØ –¢–µ–∫—É—â–∏–π –ø–æ—Ä–æ–≥: 600 —Å–µ–∫—É–Ω–¥ (300s baseline * 2.0)
# üìä Baseline –∏–∑ TC-LOAD-001: 300s (–∏–∑ README.md)
# ‚úèÔ∏è –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å: threshold = (P95 –∏–∑ TC-LOAD-001) * 2.0
_metrics_collector_003.define_slo(
    name="dag1_duration",
    threshold=600,                   # ‚¨ÖÔ∏è –ò–ó–ú–ï–ù–ò–¢–¨: P95_baseline * 2.0
    comparison="less_than"
)

# SLO #2: DAG #2 Duration –¥–ª—è Peak —Ç–µ—Å—Ç–∞ (Heavy users only)
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è PM –¥–∞—à–±–æ—Ä–¥–∞ –ø—Ä–∏ 5 Heavy + 3 Light –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
# üéØ –¢–µ–∫—É—â–∏–π –ø–æ—Ä–æ–≥: 360 —Å–µ–∫—É–Ω–¥ (180s baseline * 2.0)
# üìä Baseline –∏–∑ TC-LOAD-001: 180s (–∏–∑ README.md)
# ‚úèÔ∏è –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å: threshold = (P95 –∏–∑ TC-LOAD-001) * 2.0
_metrics_collector_003.define_slo(
    name="dag2_duration",
    threshold=360,                   # ‚¨ÖÔ∏è –ò–ó–ú–ï–ù–ò–¢–¨: P95_baseline * 2.0
    comparison="less_than"
)

# SLO #3: Dashboard Load –¥–ª—è Peak —Ç–µ—Å—Ç–∞ (Heavy users only)
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—à–±–æ—Ä–¥–∞ –ø—Ä–∏ 5 Heavy + 3 Light –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
# üéØ –¢–µ–∫—É—â–∏–π –ø–æ—Ä–æ–≥: 6.0 —Å–µ–∫—É–Ω–¥ (3s baseline * 2.0)
# üìä Baseline –∏–∑ TC-LOAD-001: 3s (–∏–∑ README.md)
# ‚úèÔ∏è –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å: threshold = (P95 –∏–∑ TC-LOAD-001) * 2.0
_metrics_collector_003.define_slo(
    name="dashboard_duration",
    threshold=6.0,                   # ‚¨ÖÔ∏è –ò–ó–ú–ï–ù–ò–¢–¨: P95_baseline * 2.0
    comparison="less_than"
)

# ============================================================================
# üìä BASELINE METRICS SETUP
# ============================================================================
# Baseline –º–µ—Ç—Ä–∏–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ config_multi.yaml
# –°–º. —Å–µ–∫—Ü–∏—é 'baseline_metrics' –≤ config —Ñ–∞–π–ª–µ
#
# –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¢–û–õ–¨–ö–û –∫ Heavy users!
# Light users –Ω–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å baseline - —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –¥–æ–ø. –Ω–∞–≥—Ä—É–∑–∫–∞
# ============================================================================


def get_metrics_collector_003() -> MetricsCollector:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π metrics collector –¥–ª—è TC-LOAD-003"""
    return _metrics_collector_003


# ============================================================================
# –°–ï–ö–¶–ò–Ø 3: HEAVY USER CLASS (ETL Operations)
# ============================================================================

class TC_LOAD_003_Heavy(LoadApi):
    """
    Heavy ETL operations - 5 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

    –°—Ü–µ–Ω–∞—Ä–∏–π:
    1. CSV Upload
    2. DAG #1 (ClickHouse Import) + –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    3. DAG #2 (PM Dashboard Creation) + –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    4. Open Dashboard (–ø—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç)
    5. –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –¥–∞—à–±–æ—Ä–¥–∞ –≤ DashboardPool –¥–ª—è Light users

    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –†–∞–±–æ—Ç–∞–µ—Ç –ù–ï–ó–ê–í–ò–°–ò–ú–û –æ—Ç –¥—Ä—É–≥–∏—Ö Heavy users (–Ω–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏)
    - –ö–∞–∂–¥—ã–π –≤ —Å–≤–æ—ë–º —Ç–µ–º–ø–µ
    - –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ MetricsCollector
    """

    wait_time = between(min_wait=1, max_wait=3)

    def __init__(self, parent):
        super().__init__(parent)
        self.user_id = f"heavy_user_{random.randint(10000, 99999)}"
        self.session_id = f"heavy_{random.randint(1000, 9999)}"

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–æ–ª—è
        self.logged_in = False
        self.session_valid = False
        self.username = None
        self.password = None
        self.flow_id = None
        self.pm_flow_id = None
        self.worker_id = 0

        # CSV –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.total_chunks = count_chunks(CONFIG["csv_file_path"], CONFIG["chunk_size"])
        self.total_lines = count_csv_lines(CONFIG["csv_file_path"])

        # ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç)
        self.ch_monitor: Optional[ClickHouseMonitor] = None
        self._init_clickhouse_monitor()

        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—á—ë—Ç–∞
        self.test_start_time = None
        self.csv_upload_duration = 0
        self.dag1_duration = 0
        self.dag2_duration = 0
        self.dashboard_duration = 0
        self.total_duration = 0

    def _init_clickhouse_monitor(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∞
        –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π Heavy user –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç
        Thread-safe —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Lock –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è race condition
        """
        ch_config = CONFIG.get("clickhouse", {})

        if not ch_config.get("enabled", False):
            self.log("[TC-LOAD-003][Heavy] ClickHouse monitoring disabled")
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ª–∏ —É–∂–µ (—Å Lock –¥–ª—è thread-safety)
        collector = get_metrics_collector_003()
        with collector.lock:
            if collector.clickhouse_monitor is not None:
                self.log("[TC-LOAD-003][Heavy] ClickHouse monitor already initialized by another user")
                return

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–æ–Ω–∏—Ç–æ—Ä–∞ –µ—â–µ –Ω–µ—Ç
            try:
                self.ch_monitor = ClickHouseMonitor(
                    host=ch_config.get("host", "localhost"),
                    port=ch_config.get("port", 8123),
                    user=ch_config.get("user", "default"),
                    password=ch_config.get("password", ""),
                    monitoring_interval=ch_config.get("monitoring_interval", 10)
                )

                if self.ch_monitor.check_connection():
                    self.log("[TC-LOAD-003][Heavy] ClickHouse monitor initialized successfully")
                    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º collector (—É–∂–µ –≤–Ω—É—Ç—Ä–∏ Lock)
                    collector.clickhouse_monitor = self.ch_monitor
                else:
                    self.log("[TC-LOAD-003][Heavy] ClickHouse connection failed, monitoring disabled", logging.WARNING)
                    self.ch_monitor = None

            except Exception as e:
                self.log(f"[TC-LOAD-003][Heavy] Failed to initialize ClickHouse monitor: {e}", logging.ERROR)
                self.ch_monitor = None

    def _format_file_size(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—á—ë—Ç–∞"""
        try:
            csv_path = CONFIG.get("csv_file_path", "")
            if csv_path and os.path.exists(csv_path):
                size_bytes = os.path.getsize(csv_path)
                size_mb = size_bytes / (1024 * 1024)
                return f"{size_mb:.1f} MB"
        except Exception:
            pass
        return "N/A"

    def _log_msg(self, message: str, level=logging.INFO):
        """Helper –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–º [TC-LOAD-003][Heavy][username]"""
        self.log(f"[TC-LOAD-003][Heavy][{self.username}] {message}", level)

    def establish_session(self):
        """Establish user session with authentication"""
        success = establish_session(
            client=self.client,
            username=self.username,
            password=self.password,
            session_id=self.session_id,
            log_function=self.log
        )

        if success:
            self.logged_in = True
            self.session_valid = True
            self.log(f"[TC-LOAD-003][Heavy] Authentication successful for {self.username}")
        else:
            self.log("[TC-LOAD-003][Heavy] Authentication failed", logging.ERROR)
            self.interrupt()

    def _register_failure(self, reason: str):
        """
        –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–µ—É–¥–∞—á–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏—è –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—Å–µ—Ö early returns —á—Ç–æ–±—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å—á–∏—Ç–∞—Ç—å success rate
        """
        get_metrics_collector_003().register_test_run({
            'success': False,
            'user_type': 'heavy',
            'username': self.username,
            'error': reason,
        })
        self._log_msg(f"Scenario failed: {reason}", logging.ERROR)

    def on_start(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Heavy user"""
        runner = getattr(self, "environment", None)
        if runner:
            runner = getattr(runner, "runner", None)
            self.worker_id = getattr(runner, "worker_id", 0) if runner else 0

        creds = UserPool.get_credentials()
        self.username = creds["username"]
        self.password = creds["password"]
        self.client.verify = False

        self.establish_session()
        self.log(f"[TC-LOAD-003][Heavy] User {self.username} started")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º collector
        get_metrics_collector_003().set_test_times(time.time(), time.time())

        # –°—Ç–∞—Ä—Ç—É–µ–º ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å)
        if self.ch_monitor:
            self.ch_monitor.collect_baseline()
            self.ch_monitor.start_monitoring()

    def on_stop(self):
        """Clean up when user stops"""
        self.log(f"[TC-LOAD-003][Heavy] User {self.username} stopped")

    @task
    def heavy_etl_scenario(self):
        """
        –û–°–ù–û–í–ù–û–ô –°–¶–ï–ù–ê–†–ò–ô HEAVY USER

        –ü–æ–ª–Ω—ã–π ETL pipeline –±–µ–∑ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ users:
        CSV Upload ‚Üí DAG#1 ‚Üí DAG#2 ‚Üí Dashboard ‚Üí Register for Light users
        """

        if not self.logged_in:
            self.establish_session()
            if not self.logged_in:
                self._register_failure("authentication_failed")
                return

        self._log_msg("Starting ETL scenario")
        self.test_start_time = time.time()
        scenario_start = time.time()

        try:
            # ========== PHASE 1: CSV Upload & File Import Flow ==========
            self._log_msg("[PHASE 1] CSV Upload & File Import")
            phase1_start = time.time()

            # 1. –°–æ–∑–¥–∞–Ω–∏–µ flow –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
            flow_name, flow_id = self._create_flow(worker_id=self.worker_id)
            self.flow_id = flow_id

            if not flow_id:
                self._register_failure("flow_creation_failed")
                return

            self._log_msg(f"File flow created: {flow_name} (ID: {flow_id})")

            # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ DAG
            target_connection, target_schema = self._get_dag_import_params(flow_id)
            if not target_connection or not target_schema:
                self._register_failure("missing_dag_parameters")
                return

            # 3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ flow –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
            update_resp = self._update_flow(
                flow_id,
                flow_name,
                target_connection,
                target_schema,
                file_uploaded=False,
                count_chunks_val=self.total_chunks,
            )
            if not update_resp or not update_resp.ok:
                self._register_failure("flow_update_failed")
                return

            # 4. –ü–æ–ª—É—á–µ–Ω–∏–µ ID –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            db_id = self._get_user_database_id()
            if not db_id:
                self._register_failure("user_database_not_found")
                return

            if self.total_chunks == 0:
                self._register_failure("no_chunks_to_upload")
                return

            timeout = (
                CONFIG["upload_control"]["timeout_large"]
                if self.total_chunks > CONFIG["upload_control"]["chunk_threshold"]
                else CONFIG["upload_control"]["timeout_small"]
            )

            # 5. –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏
            csv_upload_start = time.time()
            if not self._start_file_upload(flow_id, db_id, target_schema, self.total_chunks, timeout):
                self._register_failure("start_file_upload_failed")
                return

            # 6. –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤
            uploaded_chunks = self._upload_chunks(flow_id, db_id, target_schema, self.total_chunks)
            csv_upload_duration = time.time() - csv_upload_start
            self.csv_upload_duration = csv_upload_duration
            self._log_msg(f"CSV upload completed: {uploaded_chunks}/{self.total_chunks} chunks in {csv_upload_duration:.2f}s")

            # 7. –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
            if not self._finalize_file_upload(flow_id, uploaded_chunks, timeout):
                self._register_failure("finalize_file_upload_failed")
                return

            # ========== DAG #1: File Processing (ClickHouse Import) ==========
            self._log_msg("[PHASE 2] DAG #1: ClickHouse Import")
            dag1_start = time.time()

            # 8. –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞
            file_run_id = self._start_file_processing(
                flow_id, target_connection, target_schema, self.total_chunks, timeout
            )
            if not file_run_id:
                self._register_failure("start_file_processing_failed")
                return

            # 9. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞
            file_processing_start = time.time()
            success = self._monitor_processing_status(
                file_run_id, timeout, flow_id, db_id, target_schema,
                self.total_lines, file_processing_start, is_pm_flow=False
            )

            if not success:
                self._register_failure("dag1_processing_failed")
                return

            dag1_duration = time.time() - dag1_start
            self.dag1_duration = dag1_duration
            phase1_duration = time.time() - phase1_start
            self._log_msg(f"DAG #1 completed in {dag1_duration:.2f}s")
            self._log_msg(f"[PHASE 1] Completed in {phase1_duration:.2f}s")

            # ========== PHASE 2: Process Mining Flow ==========
            self._log_msg("[PHASE 3] DAG #2: Process Mining Dashboard")
            phase2_start = time.time()

            # 10. –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è PM –±–ª–æ–∫–∞
            source_connection, source_schema = self._get_dag_pm_params(flow_id)
            if not all([source_connection, source_schema]):
                self._register_failure("missing_pm_dag_parameters")
                return

            # 11. –°–æ–∑–¥–∞–µ–º PM flow
            table_name = f"Tube_{flow_id}"
            pm_flow_name, pm_flow_id = self._create_pm_flow(
                worker_id=self.worker_id,
                source_connection=source_connection,
                source_schema=source_schema,
                table_name=table_name,
                base_flow_name=flow_name
            )

            if not pm_flow_id:
                self._register_failure("pm_flow_creation_failed")
                return

            self.pm_flow_id = pm_flow_id
            self._log_msg(f"PM Flow created: {pm_flow_name} (ID: {pm_flow_id})")

            # 12. –ó–∞–ø—É—Å–∫–∞–µ–º Process Mining flow (DAG #2)
            dag2_start = time.time()
            pm_run_id = self._start_pm_flow(
                pm_flow_id, source_connection, source_schema, table_name
            )

            if not pm_run_id:
                self._register_failure("start_pm_flow_failed")
                return

            # 13. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç—É—Å–∞ Process Mining
            pm_timeout = CONFIG["upload_control"]["pm_timeout"]
            pm_result = self._monitor_processing_status(
                pm_run_id, pm_timeout, pm_flow_id, is_pm_flow=True
            )

            if not (isinstance(pm_result, dict) and pm_result.get("success")):
                self._register_failure("dag2_processing_failed")
                return

            dag2_duration = time.time() - dag2_start
            self.dag2_duration = dag2_duration
            self._log_msg(f"DAG #2 completed in {dag2_duration:.2f}s")

            # ========== PHASE 3: Dashboard Interaction ==========
            self._log_msg("[PHASE 4] Dashboard Interaction")

            # 14. –ü–æ–ª—É—á–∞–µ–º block_run_ids –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ–º –¥–∞—à–±–æ—Ä–¥
            block_run_ids = pm_result.get("block_run_ids", {})
            target_block_id = "spm_dashboard_creation_v_0_2[0]"
            block_run_id = block_run_ids.get(target_block_id)

            if block_run_id:
                # –ü–æ–ª—É—á–∞–µ–º URL –¥–∞—à–±–æ—Ä–¥–∞ –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
                dashboard_url = self._get_dashboard_url_from_artefacts(
                    pm_flow_id=pm_flow_id,
                    block_id=target_block_id,
                    block_run_id=block_run_id,
                    run_id=pm_run_id
                )

                if dashboard_url:
                    # –û—Ç–∫—Ä—ã–≤–∞–µ–º –¥–∞—à–±–æ—Ä–¥
                    dashboard_start = time.time()
                    dashboard_loaded = self._open_dashboard(dashboard_url)
                    dashboard_duration = time.time() - dashboard_start
                    self.dashboard_duration = dashboard_duration

                    if dashboard_loaded:
                        self._log_msg(f"Dashboard loaded in {dashboard_duration:.2f}s: {dashboard_url}")

                        # ‚úÖ –ö–õ–Æ–ß–ï–í–û–ô –ú–û–ú–ï–ù–¢: –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –¥–∞—à–±–æ—Ä–¥ –¥–ª—è Light users!
                        get_dashboard_pool_003().add(dashboard_url, self.username)
                        self._log_msg(f"Dashboard registered for Light users (total: {get_dashboard_pool_003().count()})")
                    else:
                        self._log_msg("Failed to load dashboard", logging.WARNING)
                else:
                    self._log_msg("Could not retrieve dashboard URL", logging.WARNING)
            else:
                self._log_msg(f"block_run_id not found for {target_block_id}", logging.WARNING)

            phase2_duration = time.time() - phase2_start
            self._log_msg(f"[PHASE 3] Completed in {phase2_duration:.2f}s")

            # ========== Scenario Complete ==========
            total_duration = time.time() - scenario_start
            self.total_duration = total_duration
            self._log_msg(
                f"ETL completed successfully in {total_duration:.2f}s "
                f"(CSV: {self.csv_upload_duration:.2f}s, DAG#1: {self.dag1_duration:.2f}s, DAG#2: {self.dag2_duration:.2f}s)"
            )

            # ========== –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º collector ==========
            get_metrics_collector_003().register_test_run({
                'success': True,
                'user_type': 'heavy',
                'username': self.username,
                'flow_id': self.flow_id,
                'pm_flow_id': self.pm_flow_id,
                'csv_upload_duration': self.csv_upload_duration,
                'dag1_duration': self.dag1_duration,
                'dag2_duration': self.dag2_duration,
                'dashboard_duration': self.dashboard_duration,
                'total_duration': self.total_duration,
                'file_size': self._format_file_size(),
                'total_lines': self.total_lines,
                'total_chunks': self.total_chunks,
            })

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
            get_metrics_collector_003().set_test_times(self.test_start_time, time.time())

        except Exception as e:
            self._log_msg(f"Unexpected error in ETL scenario: {str(e)}", logging.ERROR)

            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º failed run
            get_metrics_collector_003().register_test_run({
                'success': False,
                'user_type': 'heavy',
                'username': self.username,
                'error': str(e),
            })


# ============================================================================
# –°–ï–ö–¶–ò–Ø 4: LIGHT USER CLASS (Superset UI Operations)
# ============================================================================

class TC_LOAD_003_Light(LoadApi):
    """
    Light Superset UI operations - 3 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    –°—Ü–µ–Ω–∞—Ä–∏–π:
    1. –ñ–¥—É—Ç –ø–æ—è–≤–ª–µ–Ω–∏—è –¥–∞—à–±–æ—Ä–¥–æ–≤ –æ—Ç Heavy users
    2. –†–∞–±–æ—Ç–∞—é—Ç –≤ –¶–ò–ö–õ–ï:
       - –û—Ç–∫—Ä—ã–≤–∞—é—Ç –¥–∞—à–±–æ—Ä–¥—ã
       - –ü—Ä–∏–º–µ–Ω—è—é—Ç —Ñ–∏–ª—å—Ç—Ä—ã (–ó–ê–ì–õ–£–®–ö–ê)
       - –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Ç—Å—è –º–µ–∂–¥—É –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ (–ó–ê–ì–õ–£–®–ö–ê)
       - –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É—é—Ç –¥–∞–Ω–Ω—ã–µ (–ó–ê–ì–õ–£–®–ö–ê)
    3. –°–æ–∑–¥–∞—é—Ç –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ Superset UI

    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –°—Ç–∞—Ä—Ç—É—é—Ç —Å—Ä–∞–∑—É, –Ω–æ –ñ–î–£–¢ –¥–∞—à–±–æ—Ä–¥—ã
    - –†–∞–±–æ—Ç–∞—é—Ç –≤ —Ü–∏–∫–ª–µ (–Ω–µ –æ–¥–Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—è, –∞ continuous load)
    - –ò–∑–º–µ—Ä—è—é—Ç Superset response time

    TODO: –ó–∞–º–µ–Ω–∏—Ç—å –∑–∞–≥–ª—É—à–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ Superset API endpoints –∫–æ–≥–¥–∞ –æ–Ω–∏ –±—É–¥—É—Ç –≥–æ—Ç–æ–≤—ã
    """

    wait_time = between(min_wait=2, max_wait=5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏

    def __init__(self, parent):
        super().__init__(parent)
        self.user_id = f"light_user_{random.randint(10000, 99999)}"
        self.session_id = f"light_{random.randint(1000, 9999)}"

        self.logged_in = False
        self.session_valid = False
        self.username = None
        self.password = None

        # ChartApi –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∞—Ä—Ç–æ–≤
        self.chart_api: Optional[ChartApi] = None

        # –°—á—ë—Ç—á–∏–∫–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        self.dashboard_opens = 0
        self.chart_creates = 0
        self.exports = 0

        # –í—Ä–µ–º—è –æ–ø–µ—Ä–∞—Ü–∏–π
        self.dashboard_load_times = []
        self.chart_create_times = []
        self.export_times = []

    def _log_msg(self, message: str, level=logging.INFO):
        """Helper –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–º [TC-LOAD-003][Light][username]"""
        self.log(f"[TC-LOAD-003][Light][{self.username}] {message}", level)

    def establish_session(self):
        """Establish user session with authentication"""
        success = establish_session(
            client=self.client,
            username=self.username,
            password=self.password,
            session_id=self.session_id,
            log_function=self.log
        )

        if success:
            self.logged_in = True
            self.session_valid = True
            self.log(f"[TC-LOAD-003][Light] Authentication successful for {self.username}")
        else:
            self.log("[TC-LOAD-003][Light] Authentication failed", logging.ERROR)
            self.interrupt()

    def on_start(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Light user

        –í–ê–ñ–ù–û: –ñ–¥—ë–º –ø–æ—è–≤–ª–µ–Ω–∏—è –¥–∞—à–±–æ—Ä–¥–æ–≤ –æ—Ç Heavy users!
        """

        # 1. –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        creds = UserPool.get_credentials()
        self.username = creds["username"]
        self.password = creds["password"]
        self.client.verify = False

        self.establish_session()

        if not self.logged_in:
            self._log_msg("Failed to authenticate", logging.ERROR)
            self.interrupt()
            return

        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChartApi
        self.chart_api = ChartApi(self.client, self.log)

        # 3. –ñ–¥—ë–º –¥–∞—à–±–æ—Ä–¥—ã
        self._log_msg("Waiting for dashboards from Heavy users...")

        if not get_dashboard_pool_003().wait_until_available(timeout=600):
            self._log_msg(f"Timeout: No dashboards available after 10 min", logging.WARNING)
            self.interrupt()
            return

        self._log_msg(f"Dashboards available ({get_dashboard_pool_003().count()})! Starting UI operations")

    def on_stop(self):
        """
        –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã Light user
        –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞ –≤–µ—Å—å —Ç–µ—Å—Ç)
        """

        if self.dashboard_opens > 0 or self.chart_creates > 0:
            get_metrics_collector_003().register_test_run({
                'user_type': 'light',
                'username': self.username,
                'dashboard_opens': self.dashboard_opens,
                'chart_creates': self.chart_creates,
                'exports': self.exports,
                'dashboard_load_times': self.dashboard_load_times,
                'chart_create_times': self.chart_create_times,
                'export_times': self.export_times,
            })

        self._log_msg(f"Stopped. Operations: "
                 f"{self.dashboard_opens} opens, {self.chart_creates} charts, {self.exports} exports")

    @task(weight=5)
    def open_and_explore_dashboard(self):
        """
        –ó–ê–î–ê–ß–ê 1: –û—Ç–∫—Ä—ã—Ç—å –¥–∞—à–±–æ—Ä–¥ –∏ –ø–æ—Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–∏–º

        –°–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
        - –û—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–∞—à–±–æ—Ä–¥
        - –ñ–¥—ë—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        - –ò–∑–º–µ—Ä—è–µ—Ç –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞

        Weight=5: —Å–∞–º–∞—è —á–∞—Å—Ç–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è
        """

        dashboard_url = get_dashboard_pool_003().get_random()

        if not dashboard_url:
            self._log_msg("No dashboards in pool", logging.WARNING)
            return

        start_time = time.time()

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞ LoadApi
        success = self._open_dashboard(dashboard_url)
        load_time = time.time() - start_time

        if success:
            self.dashboard_load_times.append(load_time)
            self.dashboard_opens += 1
            self._log_msg(f"Dashboard loaded in {load_time:.2f}s")
        else:
            self._log_msg(f"Failed to load dashboard: {dashboard_url}", logging.WARNING)

    @task(weight=3)
    def create_chart(self):
        """
        –ó–ê–î–ê–ß–ê 2: –°–æ–∑–¥–∞—Ç—å —á–∞—Ä—Ç –Ω–∞ –¥–∞—à–±–æ—Ä–¥–µ

        –°–∏–º—É–ª–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–∞, –∫–æ—Ç–æ—Ä—ã–π:
        - –í—ã–±–∏—Ä–∞–µ—Ç –¥–∞—à–±–æ—Ä–¥
        - –ü–æ–ª—É—á–∞–µ—Ç datasource_id –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞—à–±–æ—Ä–¥–µ
        - –°–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π —á–∞—Ä—Ç (table, histogramChart –∏–ª–∏ supersetGraph)

        Weight=3: —Å—Ä–µ–¥–Ω—è—è —á–∞—Å—Ç–æ—Ç–∞
        """

        dashboard_url = get_dashboard_pool_003().get_random()

        if not dashboard_url:
            self._log_msg("No dashboards in pool for chart creation", logging.WARNING)
            return

        if not self.chart_api:
            self._log_msg("ChartApi not initialized", logging.ERROR)
            return

        start_time = time.time()

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—à–±–æ—Ä–¥–µ (–≤–∫–ª—é—á–∞—è datasource_id)
        dashboard_info = self.chart_api.get_dashboard_info(dashboard_url)

        if not dashboard_info or not dashboard_info.get('datasource_id'):
            self._log_msg(f"Could not get datasource_id from dashboard: {dashboard_url}", logging.WARNING)
            return

        datasource_id = dashboard_info['datasource_id']
        self._log_msg(f"Creating chart for datasource_id={datasource_id}")

        # –°–æ–∑–¥–∞—ë–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Ä—Ç (—Å–ª—É—á–∞–π–Ω—ã–π —Ç–∏–ø)
        success, chart_id = self.chart_api.create_and_save_chart(datasource_id)
        create_time = time.time() - start_time

        if success:
            self.chart_create_times.append(create_time)
            self.chart_creates += 1
            self._log_msg(f"Chart created in {create_time:.2f}s (chart_id={chart_id})")
        else:
            self._log_msg(f"Failed to create chart for datasource_id={datasource_id}", logging.WARNING)

    @task(weight=2)
    def export_dashboard_data(self):
        """
        –ó–ê–î–ê–ß–ê 3: –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ

        –°–∏–º—É–ª–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∫–æ—Ç–æ—Ä—ã–π:
        - –í—ã–±–∏—Ä–∞–µ—Ç –¥–∞—à–±–æ—Ä–¥
        - –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç (CSV/Excel)
        - –ñ–¥—ë—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞

        Weight=2: —Å–∞–º–∞—è —Ä–µ–¥–∫–∞—è (–Ω–æ —Ç—è–∂—ë–ª–∞—è) –æ–ø–µ—Ä–∞—Ü–∏—è

        TODO: –ó–∞–º–µ–Ω–∏—Ç—å –∑–∞–≥–ª—É—à–∫—É –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π GET/POST –∑–∞–ø—Ä–æ—Å –Ω–∞ —ç–∫—Å–ø–æ—Ä—Ç
        """

        dashboard_url = get_dashboard_pool_003().get_random()

        if not dashboard_url:
            return

        self._log_msg("Exporting data from dashboard")

        start_time = time.time()

        # === –ó–ê–ì–õ–£–®–ö–ê: –ó–¥–µ—Å—å –±—É–¥–µ—Ç GET/POST –∑–∞–ø—Ä–æ—Å –Ω–∞ —ç–∫—Å–ø–æ—Ä—Ç ===
        # TODO: –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–≥–¥–∞ –±—É–¥—É—Ç –≥–æ—Ç–æ–≤—ã endpoints
        # response = self.client.get(
        #     f"{dashboard_url}/api/export?format=csv",
        #     name="[Light] Export Data",
        #     catch_response=True
        # )

        # –ó–ê–ì–õ–£–®–ö–ê: —Å–∏–º—É–ª—è—Ü–∏—è (—ç–∫—Å–ø–æ—Ä—Ç –¥–æ–ª—å—à–µ)
        time.sleep(random.uniform(1.0, 3.0))
        export_time = time.time() - start_time
        self.export_times.append(export_time)
        self.exports += 1

        self._log_msg(f"Export completed in {export_time:.2f}s")


# ============================================================================
# –°–ï–ö–¶–ò–Ø 5: LOCUST EVENT LISTENERS
# ============================================================================

@events.test_start.add_listener
def on_test_start_003(environment, **kwargs):
    """
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ TC-LOAD-003
    –í—ã–≤–æ–¥–∏—Ç –±–∞–Ω–Ω–µ—Ä —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–µ—Å—Ç–µ
    """

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ TC-LOAD-003 –∑–∞–ø—É—â–µ–Ω
    try:
        from locustfile import SupersetUser
        if TC_LOAD_003_Heavy not in SupersetUser.tasks and TC_LOAD_003_Light not in SupersetUser.tasks:
            return  # –¢–µ—Å—Ç –Ω–µ –∑–∞–ø—É—â–µ–Ω
    except Exception:
        pass

    print("\n" + "=" * 80)
    print("TC-LOAD-003: PEAK CONCURRENT LOAD TEST STARTED")
    print("=" * 80)
    print(f"Configuration:")
    print(f"  - Test Type: Peak Concurrent")
    print(f"  - Heavy Users: 5 (ETL Pipeline - CSV ‚Üí DAG#1 ‚Üí DAG#2 ‚Üí Dashboard)")
    print(f"  - Light Users: 3 (Superset UI - open, filters, export)")
    print(f"  - Synchronization: None (all users work independently)")
    print(f"  - CSV File: {CONFIG.get('csv_file_path', 'N/A')}")
    print(f"  - ClickHouse Monitoring: {'Enabled' if CONFIG.get('clickhouse', {}).get('enabled', False) else 'Disabled'}")
    print("=" * 80 + "\n")


@events.test_stop.add_listener
def on_test_stop_003(environment, **kwargs):
    """
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ TC-LOAD-003
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç unified –æ—Ç—á—ë—Ç –∏—Å–ø–æ–ª—å–∑—É—è ReportGenerator
    """

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ TC-LOAD-003 –∑–∞–ø—É—â–µ–Ω
    try:
        from locustfile import SupersetUser
        if TC_LOAD_003_Heavy not in SupersetUser.tasks and TC_LOAD_003_Light not in SupersetUser.tasks:
            return  # –¢–µ—Å—Ç –Ω–µ –∑–∞–ø—É—â–µ–Ω
    except Exception:
        pass

    collector = get_metrics_collector_003()

    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    if collector.clickhouse_monitor:
        collector.clickhouse_monitor.stop_monitoring()
        collector.clickhouse_monitor.collect_final()

    # –°–æ–±–∏—Ä–∞–µ–º Locust stats
    stats = environment.stats
    locust_metrics = {
        'total_rps': stats.total.current_rps if stats.total.num_requests > 0 else 0,
        'total_requests': stats.total.num_requests,
        'total_failures': stats.total.num_failures,
        'median_response_time': stats.total.median_response_time,
        'avg_response_time': stats.total.avg_response_time,
        'percentile_95': stats.total.get_response_time_percentile(0.95),
        'percentile_99': stats.total.get_response_time_percentile(0.99),
    }
    collector.locust_metrics = locust_metrics

    # ============================================================================
    # üÜï –ì–ï–ù–ï–†–ê–¶–ò–Ø ENHANCED –û–¢–ß–ï–¢–û–í
    # ============================================================================
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ —Å:
    # - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ percentiles (P50, P75, P90, P95, P99)
    # - SLO compliance tracking (—Ç–æ–ª—å–∫–æ –¥–ª—è Heavy users)
    # - Baseline comparison (—Ç–æ–ª—å–∫–æ –¥–ª—è Heavy users)
    # - Separate sections –¥–ª—è Heavy –∏ Light users
    # - Multiple formats: Text, JSON, CSV
    # ============================================================================

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã —Å –ø–æ–º–æ—â—å—é ReportGenerator
    generator = ReportGenerator(collector)

    # –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª—å
    text_report = generator.generate_text_report()
    print("\n" + text_report)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç—á–µ—Ç–æ–≤ (Text, JSON, CSV)
    try:
        saved_files = generator.save_reports(output_dir="./logs")
        print(f"\n[TC-LOAD-003] ‚úì Successfully saved {len(saved_files)} report files:")
        for filepath in saved_files:
            print(f"  - {filepath}")
    except Exception as e:
        print(f"\n[TC-LOAD-003] ‚úó Failed to save reports: {e}")

    print("\n" + "=" * 80)
    print("[TC-LOAD-003] Peak Concurrent Load Test completed")
    print("=" * 80 + "\n")
