"""
TC-LOAD-001: Baseline Load Test Scenario
–ë–∞–∑–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ baseline –º–µ—Ç—Ä–∏–∫
"""

import logging
import random
import time
import urllib3
from datetime import datetime
from typing import Optional, List, Dict
from threading import Lock

from locust import task, between, events

from common.auth import establish_session
from common.API.load_api import Api
from common.csv_utils import count_chunks, count_csv_lines
from common.managers import UserPool
from common.clickhouse_monitor import ClickHouseMonitor
from common.report_engine import MetricsCollector, ReportGenerator  # üÜï –ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏
from config import CONFIG

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# ============================================================================
# üÜï ENHANCED REPORTING SYSTEM
# ============================================================================
# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π collector –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ percentiles, SLO tracking –∏ multi-format export
_metrics_collector = MetricsCollector(test_name="TC-LOAD-001")


# ============================================================================
# üìä SLO DEFINITIONS (Service Level Objectives)
# ============================================================================
# SLO = –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–∑–º–µ—Ä–∏–º—ã–µ —Ü–µ–ª–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã
# –§–æ—Ä–º–∞—Ç: collector.define_slo(metric_name, threshold, comparison)
#
# ‚öôÔ∏è –ö–ê–ö –ù–ê–°–¢–†–û–ò–¢–¨ –ü–û–°–õ–ï –ü–û–õ–£–ß–ï–ù–ò–Ø –†–ï–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–•:
# 1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ TC-LOAD-001 –ø–µ—Ä–≤—ã–π —Ä–∞–∑
# 2. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –æ—Ç—á–µ—Ç –≤ ./logs/tc_load_001_report_*.txt
# 3. –ù–∞–π–¥–∏—Ç–µ —Å–µ–∫—Ü–∏—é "PERFORMANCE METRICS" -> —Å–º–æ—Ç—Ä–∏—Ç–µ P95 (95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å)
# 4. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ threshold = P95 * 1.2 (–¥–æ–±–∞–≤–ª—è–µ–º 20% –∑–∞–ø–∞—Å)
# 5. –û–±–Ω–æ–≤–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∏–∂–µ
#
# –ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á–µ—Ç–∞:
#   –ï—Å–ª–∏ –≤ –æ—Ç—á–µ—Ç–µ –≤–∏–¥–∏—Ç–µ "P95: 285.3s" –¥–ª—è dag1_duration
#   –¢–æ threshold = 285.3 * 1.2 = 342.36 ‚âà 350 —Å–µ–∫—É–Ω–¥
#
# ============================================================================

# SLO #1: DAG #1 Duration (ClickHouse Import)
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –í—Ä–µ–º—è –∏–º–ø–æ—Ä—Ç–∞ CSV –¥–∞–Ω–Ω—ã—Ö –≤ ClickHouse
# üéØ –¢–µ–∫—É—â–∏–π –ø–æ—Ä–æ–≥: 300 —Å–µ–∫—É–Ω–¥ (5 –º–∏–Ω—É—Ç) - –∏–∑ README.md
# üìä –ì–¥–µ —Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: –æ—Ç—á–µ—Ç -> "DAG #1 Duration" -> "P95"
# ‚úèÔ∏è –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å: –∑–∞–º–µ–Ω–∏—Ç–µ 300 –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ P95 * 1.2
_metrics_collector.define_slo(
    name="dag1_duration",           # –ò–º—è –º–µ—Ç—Ä–∏–∫–∏ (–ù–ï –ú–ï–ù–Ø–¢–¨!)
    threshold=300,                   # ‚¨ÖÔ∏è –ò–ó–ú–ï–ù–ò–¢–¨ –Ω–∞ –æ—Å–Ω–æ–≤–µ P95 –∏–∑ –æ—Ç—á–µ—Ç–∞
    comparison="less_than"           # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ú–ï–ù–¨–®–ï –ø–æ—Ä–æ–≥–∞
)

# SLO #2: DAG #2 Duration (PM Dashboard Creation)
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è Process Mining –¥–∞—à–±–æ—Ä–¥–∞
# üéØ –¢–µ–∫—É—â–∏–π –ø–æ—Ä–æ–≥: 180 —Å–µ–∫—É–Ω–¥ (3 –º–∏–Ω—É—Ç—ã) - –∏–∑ README.md
# üìä –ì–¥–µ —Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: –æ—Ç—á–µ—Ç -> "DAG #2 Duration" -> "P95"
# ‚úèÔ∏è –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å: –∑–∞–º–µ–Ω–∏—Ç–µ 180 –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ P95 * 1.2
_metrics_collector.define_slo(
    name="dag2_duration",           # –ò–º—è –º–µ—Ç—Ä–∏–∫–∏ (–ù–ï –ú–ï–ù–Ø–¢–¨!)
    threshold=180,                   # ‚¨ÖÔ∏è –ò–ó–ú–ï–ù–ò–¢–¨ –Ω–∞ –æ—Å–Ω–æ–≤–µ P95 –∏–∑ –æ—Ç—á–µ—Ç–∞
    comparison="less_than"           # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ú–ï–ù–¨–®–ï –ø–æ—Ä–æ–≥–∞
)

# SLO #3: Dashboard Load Time
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—à–±–æ—Ä–¥–∞ –≤ –±—Ä–∞—É–∑–µ—Ä–µ
# üéØ –¢–µ–∫—É—â–∏–π –ø–æ—Ä–æ–≥: 3 —Å–µ–∫—É–Ω–¥—ã - –∏–∑ README.md
# üìä –ì–¥–µ —Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: –æ—Ç—á–µ—Ç -> "Dashboard Load Time" -> "P95"
# ‚úèÔ∏è –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å: –∑–∞–º–µ–Ω–∏—Ç–µ 3 –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ P95 * 1.2
_metrics_collector.define_slo(
    name="dashboard_duration",      # –ò–º—è –º–µ—Ç—Ä–∏–∫–∏ (–ù–ï –ú–ï–ù–Ø–¢–¨!)
    threshold=3,                     # ‚¨ÖÔ∏è –ò–ó–ú–ï–ù–ò–¢–¨ –Ω–∞ –æ—Å–Ω–æ–≤–µ P95 –∏–∑ –æ—Ç—á–µ—Ç–∞
    comparison="less_than"           # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ú–ï–ù–¨–®–ï –ø–æ—Ä–æ–≥–∞
)

# üí° –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï SLO (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –µ—Å–ª–∏ –Ω—É–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏

# SLO #4: CSV Upload Time
# _metrics_collector.define_slo(
#     name="csv_upload_duration",
#     threshold=60,                   # ‚¨ÖÔ∏è –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö
#     comparison="less_than"
# )

# SLO #5: Total Scenario Duration
# _metrics_collector.define_slo(
#     name="total_duration",
#     threshold=600,                  # ‚¨ÖÔ∏è –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö (10 –º–∏–Ω—É—Ç)
#     comparison="less_than"
# )

# ============================================================================
# üìå –í–ê–ñ–ù–û:
# - –ü–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ —Å —Ç–µ–∫—É—â–∏–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏ - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—Ç—á–µ—Ç
# - –ï—Å–ª–∏ SLO FAIL - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –ø–æ—Ä–æ–≥ —Å–ª–∏—à–∫–æ–º —Å—Ç—Ä–æ–≥–∏–π
# - –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö P95 –∑–Ω–∞—á–µ–Ω–∏–π
# - Target compliance: >= 95% (—Ç.–µ. 95% –∑–∞–ø—É—Å–∫–æ–≤ –¥–æ–ª–∂–Ω—ã —É–∫–ª–∞–¥—ã–≤–∞—Ç—å—Å—è –≤ SLO)
# ============================================================================


def get_metrics_collector() -> MetricsCollector:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π metrics collector"""
    return _metrics_collector


urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class TC_LOAD_001_Baseline(Api):
    """
    TC-LOAD-001: Baseline Load Test

    –°—Ü–µ–Ω–∞—Ä–∏–π:
    1. –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞ (500 –ú–ë)
    2. –ó–∞–ø—É—Å–∫ DAG #1 (–∑–∞–≥—Ä—É–∑–∫–∞ –≤ ClickHouse)
    3. –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è DAG #1
    4. –ó–∞–ø—É—Å–∫ DAG #2 (—Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –¥–∞—à–±–æ—Ä–¥–∞)
    5. –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è DAG #2
    6. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—à–±–æ—Ä–¥–∞
    7. –û—Ç–∫—Ä—ã—Ç–∏–µ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –¥–∞—à–±–æ—Ä–¥–æ–º

    –¶–µ–ª—å: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å baseline –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """

    wait_time = between(min_wait=1, max_wait=3)

    def __init__(self, parent):
        super().__init__(parent)
        self.user_id = f"baseline_user_{random.randint(10000, 99999)}"
        self.session_id = f"baseline_{random.randint(1000, 9999)}"
        self.logged_in = False
        self.session_valid = False
        self.total_chunks = count_chunks(CONFIG["csv_file_path"], CONFIG["chunk_size"])
        self.total_lines = count_csv_lines(CONFIG["csv_file_path"])
        self.worker_id = 0
        self.username = None
        self.password = None
        self.flow_id = None
        self.pm_flow_id = None

        # ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        self.ch_monitor: Optional[ClickHouseMonitor] = None
        self._init_clickhouse_monitor()

        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—á—ë—Ç–∞
        self.test_start_time = None
        self.csv_upload_duration = 0
        self.dag1_duration = 0
        self.dag2_duration = 0
        self.dashboard_duration = 0
        self.total_duration = 0

    def _init_clickhouse_monitor(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç ClickHouse –º–æ–Ω–∏—Ç–æ—Ä –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω"""
        ch_config = CONFIG.get("clickhouse", {})

        if not ch_config.get("enabled", False):
            self._log_msg("ClickHouse monitoring disabled")
            return

        try:
            self.ch_monitor = ClickHouseMonitor(
                host=ch_config.get("host", "localhost"),
                port=ch_config.get("port", 8123),
                user=ch_config.get("user", "default"),
                password=ch_config.get("password", ""),
                monitoring_interval=ch_config.get("monitoring_interval", 10)
            )

            if self.ch_monitor.check_connection():
                self._log_msg("ClickHouse monitor initialized successfully")
                # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º collector
                get_metrics_collector().set_clickhouse_monitor(self.ch_monitor)
            else:
                self._log_msg("ClickHouse connection failed, monitoring disabled", logging.WARNING)
                self.ch_monitor = None

        except Exception as e:
            self._log_msg(f"Failed to initialize ClickHouse monitor: {e}", logging.ERROR)
            self.ch_monitor = None

    def _format_file_size(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—á—ë—Ç–∞"""
        try:
            import os
            csv_path = CONFIG.get("csv_file_path", "")
            if csv_path and os.path.exists(csv_path):
                size_bytes = os.path.getsize(csv_path)
                size_mb = size_bytes / (1024 * 1024)
                return f"{size_mb:.1f} MB"
        except Exception:
            pass
        return "N/A"

    def _log_msg(self, message: str, level=logging.INFO):
        """Helper –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–º [TC-LOAD-001]"""
        self.log(f"[TC-LOAD-001] {message}", level)

    def establish_session(self):
        """Establish user session with authentication"""
        success = establish_session(
            client=self.client,
            username=self.username,
            password=self.password,
            session_id=self.session_id,
            log_function=self.log
        )

        if success:
            self.logged_in = True
            self.session_valid = True
            self._log_msg(f"Authentication successful for {self.username}")
        else:
            self._log_msg("Authentication failed", logging.ERROR)
            self.interrupt()

    def on_start(self):
        """Initialize baseline test"""
        runner = getattr(self, "environment", None)
        if runner:
            runner = getattr(runner, "runner", None)
            self.worker_id = getattr(runner, "worker_id", 0) if runner else 0

        creds = UserPool.get_credentials()
        self.username = creds["username"]
        self.password = creds["password"]
        self.client.verify = False

        self.establish_session()
        self._log_msg("Baseline test started")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º collector
        get_metrics_collector().set_test_times(time.time(), time.time())

        # –°—Ç–∞—Ä—Ç—É–µ–º ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        if self.ch_monitor:
            self.ch_monitor.collect_baseline()
            self.ch_monitor.start_monitoring()

    def on_stop(self):
        """Clean up when user stops"""
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        if self.ch_monitor:
            self.ch_monitor.stop_monitoring()
            self.ch_monitor.collect_final()

        self._log_msg("Baseline test stopped")

    @task
    def run_baseline_scenario(self):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π TC-LOAD-001:
        - CSV Upload
        - DAG #1: File import to ClickHouse
        - DAG #2: PM dashboard creation
        - Dashboard interaction
        """

        if not self.logged_in:
            self.establish_session()
            if not self.logged_in:
                self._log_msg("Failed to establish session", logging.ERROR)
                return

        self._log_msg("Starting baseline scenario")
        self.test_start_time = time.time()
        scenario_start = time.time()

        try:
            # ========== PHASE 1: CSV Upload & File Import Flow ==========
            self._log_msg("[PHASE 1] CSV Upload & File Import")
            phase1_start = time.time()

            # 1. –°–æ–∑–¥–∞–Ω–∏–µ flow –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
            flow_name, flow_id = self._create_flow(worker_id=self.worker_id)
            self.flow_id = flow_id

            if not flow_id:
                self._log_msg("Failed to create flow", logging.ERROR)
                return

            self._log_msg(f"File flow created: {flow_name} (ID: {flow_id})")

            # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ DAG
            target_connection, target_schema = self._get_dag_import_params(flow_id)
            if not target_connection or not target_schema:
                self._log_msg("Missing DAG parameters", logging.ERROR)
                return

            # 3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ flow –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
            update_resp = self._update_flow(
                flow_id,
                flow_name,
                target_connection,
                target_schema,
                file_uploaded=False,
                count_chunks_val=self.total_chunks,
            )
            if not update_resp or not update_resp.ok:
                self._log_msg("Failed to update flow before upload", logging.ERROR)
                return

            # 4. –ü–æ–ª—É—á–µ–Ω–∏–µ ID –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            db_id = self._get_user_database_id()
            if not db_id:
                self._log_msg("User database not found", logging.ERROR)
                return

            if self.total_chunks == 0:
                self._log_msg("No chunks to upload", logging.WARNING)
                return

            timeout = (
                CONFIG["upload_control"]["timeout_large"]
                if self.total_chunks > CONFIG["upload_control"]["chunk_threshold"]
                else CONFIG["upload_control"]["timeout_small"]
            )

            # 5. –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏
            csv_upload_start = time.time()
            if not self._start_file_upload(flow_id, db_id, target_schema, self.total_chunks, timeout):
                return

            # 6. –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤
            uploaded_chunks = self._upload_chunks(flow_id, db_id, target_schema, self.total_chunks)
            csv_upload_duration = time.time() - csv_upload_start
            self.csv_upload_duration = csv_upload_duration
            self._log_msg(f"CSV upload completed: {uploaded_chunks}/{self.total_chunks} chunks in {csv_upload_duration:.2f}s")

            # 7. –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
            if not self._finalize_file_upload(flow_id, uploaded_chunks, timeout):
                return

            # ========== DAG #1: File Processing (ClickHouse Import) ==========
            self._log_msg("[PHASE 2] DAG #1: ClickHouse Import")
            dag1_start = time.time()

            # 8. –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞
            file_run_id = self._start_file_processing(
                flow_id, target_connection, target_schema, self.total_chunks, timeout
            )
            if not file_run_id:
                return

            # 9. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞
            file_processing_start = time.time()
            success = self._monitor_processing_status(
                file_run_id, timeout, flow_id, db_id, target_schema,
                self.total_lines, file_processing_start, is_pm_flow=False
            )

            if not success:
                self._log_msg("DAG #1 processing failed", logging.ERROR)
                return

            dag1_duration = time.time() - dag1_start
            self.dag1_duration = dag1_duration
            phase1_duration = time.time() - phase1_start
            self._log_msg(f"DAG #1 completed in {dag1_duration:.2f}s")
            self._log_msg(f"[PHASE 1] Completed in {phase1_duration:.2f}s")

            # ========== PHASE 2: Process Mining Flow ==========
            self._log_msg("[PHASE 3] DAG #2: Process Mining Dashboard")
            phase2_start = time.time()

            # 10. –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è PM –±–ª–æ–∫–∞
            source_connection, source_schema = self._get_dag_pm_params(flow_id)
            if not all([source_connection, source_schema]):
                self._log_msg("Missing PM DAG parameters", logging.ERROR)
                return

            # 11. –°–æ–∑–¥–∞–µ–º PM flow
            table_name = f"Tube_{flow_id}"
            pm_flow_name, pm_flow_id = self._create_pm_flow(
                worker_id=self.worker_id,
                source_connection=source_connection,
                source_schema=source_schema,
                table_name=table_name,
                base_flow_name=flow_name
            )

            if not pm_flow_id:
                self._log_msg("Failed to create Process Mining flow", logging.ERROR)
                return

            self.pm_flow_id = pm_flow_id
            self._log_msg(f"PM Flow created: {pm_flow_name} (ID: {pm_flow_id})")

            # 12. –ó–∞–ø—É—Å–∫–∞–µ–º Process Mining flow (DAG #2)
            dag2_start = time.time()
            pm_run_id = self._start_pm_flow(
                pm_flow_id, source_connection, source_schema, table_name
            )

            if not pm_run_id:
                self._log_msg("Failed to start Process Mining flow", logging.ERROR)
                return

            # 13. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç—É—Å–∞ Process Mining
            pm_timeout = CONFIG["upload_control"]["pm_timeout"]
            pm_result = self._monitor_processing_status(
                pm_run_id, pm_timeout, pm_flow_id, is_pm_flow=True
            )

            if not (isinstance(pm_result, dict) and pm_result.get("success")):
                self._log_msg("DAG #2 processing failed", logging.ERROR)
                return

            dag2_duration = time.time() - dag2_start
            self.dag2_duration = dag2_duration
            self._log_msg(f"DAG #2 completed in {dag2_duration:.2f}s")

            # ========== PHASE 3: Dashboard Interaction ==========
            self._log_msg("[PHASE 4] Dashboard Interaction")

            # 14. –ü–æ–ª—É—á–∞–µ–º block_run_ids –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ–º –¥–∞—à–±–æ—Ä–¥
            block_run_ids = pm_result.get("block_run_ids", {})
            target_block_id = "spm_dashboard_creation_v_0_2[0]"
            block_run_id = block_run_ids.get(target_block_id)

            if block_run_id:
                # –ü–æ–ª—É—á–∞–µ–º URL –¥–∞—à–±–æ—Ä–¥–∞ –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
                dashboard_url = self._get_dashboard_url_from_artefacts(
                    pm_flow_id=pm_flow_id,
                    block_id=target_block_id,
                    block_run_id=block_run_id,
                    run_id=pm_run_id
                )

                if dashboard_url:
                    # –û—Ç–∫—Ä—ã–≤–∞–µ–º –¥–∞—à–±–æ—Ä–¥
                    dashboard_start = time.time()
                    dashboard_loaded = self._open_dashboard(dashboard_url)
                    dashboard_duration = time.time() - dashboard_start
                    self.dashboard_duration = dashboard_duration

                    if dashboard_loaded:
                        self._log_msg(f"Dashboard loaded in {dashboard_duration:.2f}s: {dashboard_url}")
                    else:
                        self._log_msg("Failed to load dashboard", logging.WARNING)
                else:
                    self._log_msg("Could not retrieve dashboard URL", logging.WARNING)
            else:
                self._log_msg(f"block_run_id not found for {target_block_id}", logging.WARNING)

            phase2_duration = time.time() - phase2_start
            self._log_msg(f"[PHASE 3] Completed in {phase2_duration:.2f}s")

            # ========== Scenario Complete ==========
            total_duration = time.time() - scenario_start
            self.total_duration = total_duration
            self._log_msg(
                f"Baseline scenario completed successfully in {total_duration:.2f}s "
                f"(CSV: {self.csv_upload_duration:.2f}s, DAG#1: {self.dag1_duration:.2f}s, DAG#2: {self.dag2_duration:.2f}s)"
            )

            # ========== –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º collector ==========
            get_metrics_collector().register_test_run({
                'success': True,
                'username': self.username,
                'flow_id': self.flow_id,
                'pm_flow_id': self.pm_flow_id,
                'csv_upload_duration': self.csv_upload_duration,
                'dag1_duration': self.dag1_duration,
                'dag2_duration': self.dag2_duration,
                'dashboard_duration': self.dashboard_duration,
                'total_duration': self.total_duration,
                'file_size': self._format_file_size(),
                'total_lines': self.total_lines,
                'total_chunks': self.total_chunks,
            })

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
            get_metrics_collector().set_test_times(self.test_start_time, time.time())

        except Exception as e:
            self._log_msg(f"Unexpected error in baseline scenario: {str(e)}", logging.ERROR)

            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º failed run
            get_metrics_collector().register_test_run({
                'success': False,
                'username': self.username,
                'error': str(e),
            })


# ========== Locust Event Listeners ==========

@events.test_stop.add_listener
def on_test_stop(environment, **kwargs):
    """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ç–µ—Å—Ç–∞ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±—â–∏–π –æ—Ç—á—ë—Ç"""

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ TC-LOAD-001 –∑–∞–ø—É—â–µ–Ω
    try:
        from locustfile import SupersetUser
        if TC_LOAD_001_Baseline not in SupersetUser.tasks:
            return  # –≠—Ç–æ—Ç —Ç–µ—Å—Ç –Ω–µ –∑–∞–ø—É—â–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    except Exception:
        return  # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–¥—Ä—É–≥–æ–π —Ç–µ—Å—Ç –∑–∞–ø—É—â–µ–Ω)

    collector = get_metrics_collector()

    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –µ—Å–ª–∏ –µ—Å—Ç—å
    if collector.clickhouse_monitor:
        collector.clickhouse_monitor.stop_monitoring()
        collector.clickhouse_monitor.collect_final()

    # –°–æ–±–∏—Ä–∞–µ–º Locust stats –¥–ª—è RPS –∏ Response Time
    stats = environment.stats
    locust_metrics = {
        'total_rps': stats.total.current_rps if stats.total.num_requests > 0 else 0,
        'total_requests': stats.total.num_requests,
        'total_failures': stats.total.num_failures,
        'median_response_time': stats.total.median_response_time,
        'avg_response_time': stats.total.avg_response_time,
        'percentile_95': stats.total.get_response_time_percentile(0.95),
        'percentile_99': stats.total.get_response_time_percentile(0.99),
    }
    collector.locust_metrics = locust_metrics

    # ============================================================================
    # üÜï –ì–ï–ù–ï–†–ê–¶–ò–Ø ENHANCED –û–¢–ß–ï–¢–û–í
    # ============================================================================
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ —Å:
    # - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ percentiles (P50, P75, P90, P95, P99)
    # - SLO compliance tracking
    # - Error analysis
    # - Smart recommendations
    # - Multiple formats: Text, JSON, CSV
    # ============================================================================

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã —Å –ø–æ–º–æ—â—å—é ReportGenerator
    generator = ReportGenerator(collector)

    # –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª—å
    text_report = generator.generate_text_report()
    print("\n" + text_report)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç—á–µ—Ç–æ–≤ (Text, JSON, CSV)
    try:
        saved_files = generator.save_reports(output_dir="./logs")
        print(f"\n[TC-LOAD-001] ‚úì Successfully saved {len(saved_files)} report files:")
        for filepath in saved_files:
            print(f"  - {filepath}")
        print()
    except Exception as e:
        print(f"\n[TC-LOAD-001] ‚úó Failed to save reports: {e}\n")
