"""
TC-LOAD-002: Concurrent Load Test (3 Users)
–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ 3 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ - —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä—É–∑–∫–∞
"""

import logging
import random
import time
import urllib3
from datetime import datetime
from typing import Optional, List, Dict
from threading import Lock

from locust import task, between, events

from common.auth import establish_session
from common.api.load_api import LoadApi
from common.csv_utils import count_chunks, count_csv_lines
from common.managers import UserPool
from common.clickhouse_monitor import ClickHouseMonitor
from common.report_engine import MetricsCollector, ReportGenerator  # üÜï –ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏
from config import CONFIG

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# ============================================================================
# üÜï ENHANCED REPORTING SYSTEM
# ============================================================================
# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π collector –¥–ª—è TC-LOAD-002 (Concurrent Test)
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ percentiles, SLO tracking –∏ baseline comparison
_metrics_collector = MetricsCollector(test_name="TC-LOAD-002")


# ============================================================================
# üìä SLO DEFINITIONS FOR TC-LOAD-002 (Concurrent Test)
# ============================================================================
# TC-LOAD-002 –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ 3 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
# SLO –∫—Ä–∏—Ç–µ—Ä–∏–π –∏–∑ README.md: "–ù–µ –±–æ–ª–µ–µ +50% –æ—Ç baseline –º–µ—Ç—Ä–∏–∫"
#
# ‚öôÔ∏è –ö–ê–ö –ù–ê–°–¢–†–û–ò–¢–¨ –ü–û–°–õ–ï –ü–û–õ–£–ß–ï–ù–ò–Ø –†–ï–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–•:
#
# –®–ê–ì–ò –ù–ê–°–¢–†–û–ô–ö–ò:
# 1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ TC-LOAD-001 –∏ –ø–æ–ª—É—á–∏—Ç–µ baseline –º–µ—Ç—Ä–∏–∫–∏
# 2. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤ –æ—Ç—á–µ—Ç–µ TC-LOAD-001 –∑–Ω–∞—á–µ–Ω–∏—è P95 –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏
# 3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ SLO –¥–ª—è TC-LOAD-002 = P95_baseline * 1.5 (–¥–æ–±–∞–≤–ª—è–µ–º 50% –∫–∞–∫ –≤ README)
# 4. –û–±–Ω–æ–≤–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∏–∂–µ
#
# –ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á–µ—Ç–∞:
#   TC-LOAD-001 –æ—Ç—á–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç "DAG #1 P95: 280.5s"
#   TC-LOAD-002 SLO = 280.5 * 1.5 = 420.75 ‚âà 425 —Å–µ–∫—É–Ω–¥
#   –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç: –ø—Ä–∏ 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö –¥–æ–ø—É—Å—Ç–∏–º–æ –∑–∞–º–µ–¥–ª–µ–Ω–∏–µ –¥–æ +50%
#
# üìå –í–ê–ñ–ù–û: –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ TC-LOAD-001 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è baseline!
# ============================================================================

# SLO #1: DAG #1 Duration –¥–ª—è Concurrent —Ç–µ—Å—Ç–∞
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –í—Ä–µ–º—è –∏–º–ø–æ—Ä—Ç–∞ CSV –≤ ClickHouse –ø—Ä–∏ 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
# üéØ –û–±–Ω–æ–≤–ª–µ–Ω–æ: 2025-12-16 –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–≥–æ baseline TC-LOAD-001
# üìä Baseline –∏–∑ TC-LOAD-001: 55.6s P95 (—Å—Ä–µ–¥–Ω–µ–µ –∏–∑ 8 –∑–∞–ø—É—Å–∫–æ–≤)
# ‚úèÔ∏è –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å: threshold = (P95 –∏–∑ TC-LOAD-001) * 1.5
_metrics_collector.define_slo(
    name="dag1_duration",
    threshold=84.0,                  # P95_baseline (55.6s) √ó 1.5 = 83.4s ‚âà 84.0s
    comparison="less_than"
)

# SLO #2: DAG #2 Duration –¥–ª—è Concurrent —Ç–µ—Å—Ç–∞
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è PM –¥–∞—à–±–æ—Ä–¥–∞ –ø—Ä–∏ 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
# üéØ –û–±–Ω–æ–≤–ª–µ–Ω–æ: 2025-12-16 –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–≥–æ baseline TC-LOAD-001
# üìä Baseline –∏–∑ TC-LOAD-001: 106.4s P95 (—Å—Ä–µ–¥–Ω–µ–µ –∏–∑ 8 –∑–∞–ø—É—Å–∫–æ–≤)
# ‚úèÔ∏è –ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å: threshold = (P95 –∏–∑ TC-LOAD-001) * 1.5
_metrics_collector.define_slo(
    name="dag2_duration",
    threshold=159.0,                 # P95_baseline (106.4s) √ó 1.5 = 159.6s ‚âà 159.0s
    comparison="less_than"
)

# SLO #3: Dashboard Load –¥–ª—è Concurrent —Ç–µ—Å—Ç–∞
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—à–±–æ—Ä–¥–∞ –ø—Ä–∏ 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
# üéØ –û–±–Ω–æ–≤–ª–µ–Ω–æ: 2025-12-16 –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–≥–æ baseline TC-LOAD-001
# üìä –í–ê–ñ–ù–û: –ú–µ—Ç—Ä–∏–∫–∞ –∫—Ä–∞–π–Ω–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞ –≤ baseline!
#    - –†–µ–∞–ª—å–Ω—ã–π P95 baseline: 1.3s
#    - –î–∏–∞–ø–∞–∑–æ–Ω baseline: 0.6s-8.4s (14√ó –≤–∞—Ä–∏–∞—Ü–∏—è!)
#    - –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π baseline: 2.5s (1.92√ó –æ—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ P95)
#    - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–Ω–æ–∂–∏—Ç–µ–ª—å –æ—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ P95: 2.92√ó (–≤–º–µ—Å—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö 1.5√ó)
# ‚úèÔ∏è –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: –í—ã—Å–æ–∫–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å + concurrent –Ω–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–±—É—é—Ç –º—è–≥–∫–æ–≥–æ –ø–æ—Ä–æ–≥–∞
_metrics_collector.define_slo(
    name="dashboard_duration",
    threshold=3.8,                   # Real P95 (1.3s) ‚Üí Safe baseline (2.5s) √ó 1.5 = 3.75s ‚âà 3.8s
    comparison="less_than"
)

# SLO #4: CSV Upload Time –¥–ª—è Concurrent —Ç–µ—Å—Ç–∞
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ CSV —Ñ–∞–π–ª–∞ –ø—Ä–∏ 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
# üéØ –û–±–Ω–æ–≤–ª–µ–Ω–æ: 2025-12-16 –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–≥–æ baseline TC-LOAD-001
# üìä Baseline –∏–∑ TC-LOAD-001: 68.2s P95 (—Å—Ä–µ–¥–Ω–µ–µ –∏–∑ 8 –∑–∞–ø—É—Å–∫–æ–≤, –∏—Å–∫–ª—é—á–µ–Ω—ã –∞–Ω–æ–º–∞–ª–∏–∏)
_metrics_collector.define_slo(
    name="csv_upload_duration",
    threshold=102.0,                 # P95_baseline (68.2s) √ó 1.5 = 102.3s ‚âà 102.0s
    comparison="less_than"
)

# SLO #5: Total Scenario Duration –¥–ª—è Concurrent —Ç–µ—Å—Ç–∞
# üìù –û–ø–∏—Å–∞–Ω–∏–µ: –ü–æ–ª–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å—Ü–µ–Ω–∞—Ä–∏—è –ø—Ä–∏ 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
# üéØ –û–±–Ω–æ–≤–ª–µ–Ω–æ: 2025-12-16 –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–≥–æ baseline TC-LOAD-001
# üìä Baseline –∏–∑ TC-LOAD-001: 227.9s P95 (—Å—Ä–µ–¥–Ω–µ–µ –∏–∑ 8 –∑–∞–ø—É—Å–∫–æ–≤)
_metrics_collector.define_slo(
    name="total_duration",
    threshold=342.0,                 # P95_baseline (227.9s) √ó 1.5 = 341.85s ‚âà 342.0s
    comparison="less_than"
)

# ============================================================================
# üìä BASELINE METRICS SETUP
# ============================================================================
# Baseline –º–µ—Ç—Ä–∏–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ config_multi.yaml
# –°–º. —Å–µ–∫—Ü–∏—é 'baseline_metrics' –≤ config —Ñ–∞–π–ª–µ
#
# –ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ TC-LOAD-001 –æ–±–Ω–æ–≤–∏—Ç–µ config_multi.yaml:
# baseline_metrics:
#   "500mb":
#     csv_upload: <–∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ TC-LOAD-001>
#     dag1_duration: <–∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ TC-LOAD-001>
#     dag2_duration: <–∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ TC-LOAD-001>
#     dashboard_load: <–∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ TC-LOAD-001>
# ============================================================================


def get_metrics_collector_002() -> MetricsCollector:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π metrics collector –¥–ª—è TC-LOAD-002"""
    return _metrics_collector


class TC_LOAD_002_Concurrent(LoadApi):
    """
    TC-LOAD-002: Concurrent Load Test

    –°—Ü–µ–Ω–∞—Ä–∏–π:
    - 3 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞—é—Ç CSV
    - –ö–∞–∂–¥—ã–π –∑–∞–ø—É—Å–∫–∞–µ—Ç DAG #1 (ClickHouse import)
    - –ö–∞–∂–¥—ã–π –∑–∞–ø—É—Å–∫–∞–µ—Ç DAG #2 (PM dashboard)
    - –ö–∞–∂–¥—ã–π –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç —Å–≤–æ–π –¥–∞—à–±–æ—Ä–¥

    –¶–µ–ª—å: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É —Å–∏—Å—Ç–µ–º—ã –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    """

    wait_time = between(min_wait=1, max_wait=3)

    def __init__(self, parent):
        super().__init__(parent)
        self.user_id = f"concurrent_user_{random.randint(10000, 99999)}"
        self.session_id = f"concurrent_{random.randint(1000, 9999)}"
        self.logged_in = False
        self.session_valid = False
        self.total_chunks = count_chunks(CONFIG["csv_file_path"], CONFIG["chunk_size"])
        self.total_lines = count_csv_lines(CONFIG["csv_file_path"])
        self.worker_id = 0
        self.username = None
        self.password = None
        self.flow_id = None
        self.pm_flow_id = None

        # ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç)
        self.ch_monitor: Optional[ClickHouseMonitor] = None
        self._init_clickhouse_monitor()

        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—á—ë—Ç–∞
        self.test_start_time = None
        self.csv_upload_duration = 0
        self.dag1_duration = 0
        self.dag2_duration = 0
        self.dashboard_duration = 0
        self.total_duration = 0

    def _init_clickhouse_monitor(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç ClickHouse –º–æ–Ω–∏—Ç–æ—Ä –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å)"""
        ch_config = CONFIG.get("clickhouse", {})

        if not ch_config.get("enabled", False):
            self.log("[TC-LOAD-002] ClickHouse monitoring disabled")
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ª–∏ —É–∂–µ
        if get_metrics_collector_002().clickhouse_monitor is not None:
            self.log("[TC-LOAD-002] ClickHouse monitor already initialized by another user")
            return

        try:
            self.ch_monitor = ClickHouseMonitor(
                host=ch_config.get("host", "localhost"),
                port=ch_config.get("port", 8123),
                user=ch_config.get("user", "default"),
                password=ch_config.get("password", ""),
                monitoring_interval=ch_config.get("monitoring_interval", 10)
            )

            if self.ch_monitor.check_connection():
                self.log("[TC-LOAD-002] ClickHouse monitor initialized successfully")
                # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º collector
                get_metrics_collector_002().set_clickhouse_monitor(self.ch_monitor)
            else:
                self.log("[TC-LOAD-002] ClickHouse connection failed, monitoring disabled", logging.WARNING)
                self.ch_monitor = None

        except Exception as e:
            self.log(f"[TC-LOAD-002] Failed to initialize ClickHouse monitor: {e}", logging.ERROR)
            self.ch_monitor = None

    def _format_file_size(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—á—ë—Ç–∞"""
        try:
            import os
            csv_path = CONFIG.get("csv_file_path", "")
            if csv_path and os.path.exists(csv_path):
                size_bytes = os.path.getsize(csv_path)
                size_mb = size_bytes / (1024 * 1024)
                return f"{size_mb:.1f} MB"
        except Exception:
            pass
        return "N/A"

    def _log_msg(self, message: str, level=logging.INFO):
        """Helper –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–º [TC-LOAD-002][username]"""
        self.log(f"[TC-LOAD-002][{self.username}] {message}", level)

    def _register_failure(self, reason: str):
        """
        –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–µ—É–¥–∞—á–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏—è –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—Å–µ—Ö early returns —á—Ç–æ–±—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å—á–∏—Ç–∞—Ç—å success rate
        """
        get_metrics_collector_002().register_test_run({
            'success': False,
            'username': self.username,
            'error': reason,
            'csv_upload_duration': self.csv_upload_duration,
            'dag1_duration': self.dag1_duration,
            'dag2_duration': self.dag2_duration,
        })
        self._log_msg(f"Scenario failed: {reason}", logging.ERROR)

    def establish_session(self):
        """Establish user session with authentication"""
        success = establish_session(
            client=self.client,
            username=self.username,
            password=self.password,
            session_id=self.session_id,
            log_function=self.log
        )

        if success:
            self.logged_in = True
            self.session_valid = True
            self.log(f"[TC-LOAD-002] Authentication successful for {self.username}")
        else:
            self.log("[TC-LOAD-002] Authentication failed", logging.ERROR)
            self.interrupt()

    def on_start(self):
        """Initialize concurrent test"""
        runner = getattr(self, "environment", None)
        if runner:
            runner = getattr(runner, "runner", None)
            self.worker_id = getattr(runner, "worker_id", 0) if runner else 0

        creds = UserPool.get_credentials()
        self.username = creds["username"]
        self.password = creds["password"]
        self.client.verify = False

        self.establish_session()
        self.log(f"[TC-LOAD-002] Concurrent test started for user: {self.username}")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º collector
        get_metrics_collector_002().set_test_times(time.time(), time.time())

        # –°—Ç–∞—Ä—Ç—É–µ–º ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å)
        if self.ch_monitor:
            self.ch_monitor.collect_baseline()
            self.ch_monitor.start_monitoring()

    def on_stop(self):
        """Clean up when user stops"""
        self.log(f"[TC-LOAD-002] User {self.username} stopped")

    @task
    def run_concurrent_scenario(self):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π TC-LOAD-002:
        - CSV Upload
        - DAG #1: File import to ClickHouse
        - DAG #2: PM dashboard creation
        - Dashboard interaction
        """

        if not self.logged_in:
            self.establish_session()
            if not self.logged_in:
                self._register_failure("authentication_failed")
                return

        self._log_msg("Starting concurrent scenario")
        self.test_start_time = time.time()
        scenario_start = time.time()

        try:
            # ========== PHASE 1: CSV Upload & File Import Flow ==========
            self._log_msg("[PHASE 1] CSV Upload & File Import")
            phase1_start = time.time()

            # 1. –°–æ–∑–¥–∞–Ω–∏–µ flow –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
            flow_name, flow_id = self._create_flow(worker_id=self.worker_id)
            self.flow_id = flow_id

            if not flow_id:
                self._register_failure("flow_creation_failed")
                return

            self._log_msg(f"File flow created: {flow_name} (ID: {flow_id})")

            # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ DAG
            target_connection, target_schema = self._get_dag_import_params(flow_id)
            if not target_connection or not target_schema:
                self._register_failure("missing_dag_parameters")
                return

            # 3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ flow –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
            update_resp = self._update_flow(
                flow_id,
                flow_name,
                target_connection,
                target_schema,
                file_uploaded=False,
                count_chunks_val=self.total_chunks,
            )
            if not update_resp or not update_resp.ok:
                self._register_failure("flow_update_failed")
                return

            # 4. –ü–æ–ª—É—á–µ–Ω–∏–µ ID –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            db_id = self._get_user_database_id()
            if not db_id:
                self._register_failure("user_database_not_found")
                return

            if self.total_chunks == 0:
                self._register_failure("no_chunks_to_upload")
                return

            timeout = (
                CONFIG["upload_control"]["timeout_large"]
                if self.total_chunks > CONFIG["upload_control"]["chunk_threshold"]
                else CONFIG["upload_control"]["timeout_small"]
            )

            # 5. –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏
            csv_upload_start = time.time()
            if not self._start_file_upload(flow_id, db_id, target_schema, self.total_chunks, timeout):
                self._register_failure("start_file_upload_failed")
                return

            # 6. –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤
            uploaded_chunks = self._upload_chunks(flow_id, db_id, target_schema, self.total_chunks)
            csv_upload_duration = time.time() - csv_upload_start
            self.csv_upload_duration = csv_upload_duration
            self._log_msg(f"CSV upload completed: {uploaded_chunks}/{self.total_chunks} chunks in {csv_upload_duration:.2f}s")

            # 7. –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
            if not self._finalize_file_upload(flow_id, uploaded_chunks, timeout):
                self._register_failure("finalize_file_upload_failed")
                return

            # ========== DAG #1: File Processing (ClickHouse Import) ==========
            self._log_msg("[PHASE 2] DAG #1: ClickHouse Import")
            dag1_start = time.time()

            # 8. –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞
            file_run_id = self._start_file_processing(
                flow_id, target_connection, target_schema, self.total_chunks, timeout
            )
            if not file_run_id:
                self._register_failure("start_file_processing_failed")
                return

            # 9. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞
            file_processing_start = time.time()
            success = self._monitor_processing_status(
                file_run_id, timeout, flow_id, db_id, target_schema,
                self.total_lines, file_processing_start, is_pm_flow=False
            )

            if not success:
                self._register_failure("dag1_processing_failed")
                return

            dag1_duration = time.time() - dag1_start
            self.dag1_duration = dag1_duration
            phase1_duration = time.time() - phase1_start
            self._log_msg(f"DAG #1 completed in {dag1_duration:.2f}s")
            self._log_msg(f"[PHASE 1] Completed in {phase1_duration:.2f}s")

            # ========== PHASE 2: Process Mining Flow ==========
            self._log_msg("[PHASE 3] DAG #2: Process Mining Dashboard")
            phase2_start = time.time()

            # 10. –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è PM –±–ª–æ–∫–∞
            source_connection, source_schema = self._get_dag_pm_params(flow_id)
            if not all([source_connection, source_schema]):
                self._register_failure("missing_pm_dag_parameters")
                return

            # 11. –°–æ–∑–¥–∞–µ–º PM flow
            table_name = f"Tube_{flow_id}"
            pm_flow_name, pm_flow_id = self._create_pm_flow(
                worker_id=self.worker_id,
                source_connection=source_connection,
                source_schema=source_schema,
                table_name=table_name,
                base_flow_name=flow_name
            )

            if not pm_flow_id:
                self._register_failure("pm_flow_creation_failed")
                return

            self.pm_flow_id = pm_flow_id
            self._log_msg(f"PM Flow created: {pm_flow_name} (ID: {pm_flow_id})")

            # 12. –ó–∞–ø—É—Å–∫–∞–µ–º Process Mining flow (DAG #2)
            dag2_start = time.time()
            pm_run_id = self._start_pm_flow(
                pm_flow_id, source_connection, source_schema, table_name
            )

            if not pm_run_id:
                self._register_failure("start_pm_flow_failed")
                return

            # 13. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç—É—Å–∞ Process Mining
            pm_timeout = CONFIG["upload_control"]["pm_timeout"]
            pm_result = self._monitor_processing_status(
                pm_run_id, pm_timeout, pm_flow_id, is_pm_flow=True
            )

            if not (isinstance(pm_result, dict) and pm_result.get("success")):
                self._register_failure("dag2_processing_failed")
                return

            dag2_duration = time.time() - dag2_start
            self.dag2_duration = dag2_duration
            self._log_msg(f"DAG #2 completed in {dag2_duration:.2f}s")

            # ========== PHASE 3: Dashboard Interaction ==========
            self._log_msg("[PHASE 4] Dashboard Interaction")

            # 14. –ü–æ–ª—É—á–∞–µ–º block_run_ids –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ–º –¥–∞—à–±–æ—Ä–¥
            block_run_ids = pm_result.get("block_run_ids", {})
            target_block_id = "spm_dashboard_creation_v_0_2[0]"
            block_run_id = block_run_ids.get(target_block_id)

            if block_run_id:
                # –ü–æ–ª—É—á–∞–µ–º URL –¥–∞—à–±–æ—Ä–¥–∞ –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
                dashboard_url = self._get_dashboard_url_from_artefacts(
                    pm_flow_id=pm_flow_id,
                    block_id=target_block_id,
                    block_run_id=block_run_id,
                    run_id=pm_run_id
                )

                if dashboard_url:
                    # –û—Ç–∫—Ä—ã–≤–∞–µ–º –¥–∞—à–±–æ—Ä–¥
                    dashboard_start = time.time()
                    dashboard_loaded = self._open_dashboard(dashboard_url)
                    dashboard_duration = time.time() - dashboard_start
                    self.dashboard_duration = dashboard_duration

                    if dashboard_loaded:
                        self._log_msg(f"Dashboard loaded in {dashboard_duration:.2f}s: {dashboard_url}")
                    else:
                        self._log_msg("Failed to load dashboard", logging.WARNING)
                else:
                    self._log_msg("Could not retrieve dashboard URL", logging.WARNING)
            else:
                self._log_msg(f"block_run_id not found for {target_block_id}", logging.WARNING)

            phase2_duration = time.time() - phase2_start
            self._log_msg(f"[PHASE 3] Completed in {phase2_duration:.2f}s")

            # ========== Scenario Complete ==========
            total_duration = time.time() - scenario_start
            self.total_duration = total_duration
            self._log_msg(
                f"Concurrent scenario completed successfully in {total_duration:.2f}s "
                f"(CSV: {self.csv_upload_duration:.2f}s, DAG#1: {self.dag1_duration:.2f}s, DAG#2: {self.dag2_duration:.2f}s)"
            )

            # ========== –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º collector ==========
            get_metrics_collector_002().register_test_run({
                'success': True,
                'username': self.username,
                'flow_id': self.flow_id,
                'pm_flow_id': self.pm_flow_id,
                'csv_upload_duration': self.csv_upload_duration,
                'dag1_duration': self.dag1_duration,
                'dag2_duration': self.dag2_duration,
                'dashboard_duration': self.dashboard_duration,
                'total_duration': self.total_duration,
                'file_size': self._format_file_size(),
                'total_lines': self.total_lines,
                'total_chunks': self.total_chunks,
            })

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
            get_metrics_collector_002().set_test_times(self.test_start_time, time.time())

        except Exception as e:
            self._log_msg(f"Unexpected error in concurrent scenario: {str(e)}", logging.ERROR)

            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º failed run
            get_metrics_collector_002().register_test_run({
                'success': False,
                'username': self.username,
                'error': str(e),
            })


# ========== Locust Event Listeners ==========

@events.test_stop.add_listener
def on_test_stop_002(environment, **kwargs):
    """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ TC-LOAD-002 - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±—â–∏–π –æ—Ç—á—ë—Ç"""

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ TC-LOAD-002 –∑–∞–ø—É—â–µ–Ω
    try:
        from locustfile import SupersetUser
        if TC_LOAD_002_Concurrent not in SupersetUser.tasks:
            return  # –≠—Ç–æ—Ç —Ç–µ—Å—Ç –Ω–µ –∑–∞–ø—É—â–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    except Exception:
        return  # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–¥—Ä—É–≥–æ–π —Ç–µ—Å—Ç –∑–∞–ø—É—â–µ–Ω)

    collector = get_metrics_collector_002()

    # ============================================================================
    # üìä –ó–ê–ì–†–£–ó–ö–ê BASELINE METRICS
    # ============================================================================
    # –ó–∞–≥—Ä—É–∂–∞–µ–º baseline –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ config_multi.yaml –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–≤–∏–¥–µ—Ç—å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç TC-LOAD-001 baseline
    # ============================================================================
    if collector.baseline_metrics is None:
        baseline_config = CONFIG.get('baseline_metrics', {})
        if baseline_config:
            try:
                import os
                csv_path = CONFIG.get("csv_file_path", "")
                if csv_path and os.path.exists(csv_path):
                    size_mb = os.path.getsize(csv_path) / (1024 * 1024)

                    # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π baseline –ø–æ —Ä–∞–∑–º–µ—Ä—É —Ñ–∞–π–ª–∞
                    selected_baseline = None
                    min_diff = float('inf')

                    for key, baseline in baseline_config.items():
                        baseline_size = baseline.get('file_size_mb', 0)
                        diff = abs(size_mb - baseline_size)
                        if diff < min_diff:
                            min_diff = diff
                            selected_baseline = baseline

                    if selected_baseline:
                        collector.set_baseline_metrics(selected_baseline)
                        print(f"[TC-LOAD-002] Loaded baseline metrics from config: {selected_baseline}")
            except Exception as e:
                print(f"[TC-LOAD-002] Warning: Could not load baseline metrics: {e}")

    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ClickHouse –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –µ—Å–ª–∏ –µ—Å—Ç—å
    if collector.clickhouse_monitor:
        collector.clickhouse_monitor.stop_monitoring()
        collector.clickhouse_monitor.collect_final()

    # –°–æ–±–∏—Ä–∞–µ–º Locust stats –¥–ª—è RPS –∏ Response Time
    stats = environment.stats
    locust_metrics = {
        'total_rps': stats.total.current_rps if stats.total.num_requests > 0 else 0,
        'total_requests': stats.total.num_requests,
        'total_failures': stats.total.num_failures,
        'median_response_time': stats.total.median_response_time,
        'avg_response_time': stats.total.avg_response_time,
        'percentile_95': stats.total.get_response_time_percentile(0.95),
        'percentile_99': stats.total.get_response_time_percentile(0.99),
    }
    collector.locust_metrics = locust_metrics

    # ============================================================================
    # üÜï –ì–ï–ù–ï–†–ê–¶–ò–Ø ENHANCED –û–¢–ß–ï–¢–û–í
    # ============================================================================
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ —Å:
    # - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ percentiles (P50, P75, P90, P95, P99)
    # - SLO compliance tracking
    # - Baseline comparison (–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç TC-LOAD-001)
    # - Error analysis
    # - Smart recommendations
    # - Per-user breakdown
    # - Multiple formats: Text, JSON, CSV
    # ============================================================================

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã —Å –ø–æ–º–æ—â—å—é ReportGenerator
    generator = ReportGenerator(collector)

    # –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª—å
    text_report = generator.generate_text_report()
    print("\n" + text_report)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç—á–µ—Ç–æ–≤ (Text, JSON, CSV)
    try:
        saved_files = generator.save_reports(output_dir="./logs")
        print(f"\n[TC-LOAD-002] ‚úì Successfully saved {len(saved_files)} report files:")
        for filepath in saved_files:
            print(f"  - {filepath}")
        print()
    except Exception as e:
        print(f"\n[TC-LOAD-002] ‚úó Failed to save reports: {e}\n")
